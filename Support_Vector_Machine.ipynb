{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Support_Vector_Machine.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/MorganGautherot/Machine_Learning_Courses/blob/master/Support_Vector_Machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"QVRfyCS23vOZ"},"source":["# Support Vector Machine"]},{"cell_type":"markdown","metadata":{"id":"hUll1EwwpE5F"},"source":["## 1 Linear Regression VS SVR"]},{"cell_type":"markdown","metadata":{"id":"zcTY-8NNpKS_"},"source":["You will see the difference between linear regression and SVM regressor"]},{"cell_type":"code","metadata":{"id":"q9ujfr9upNi7"},"source":["m = 100\n","X = 6 * np.random.rand(m, 1) - 3\n","y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n","\n","plt.scatter(X,y)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ILh57oPopZnI"},"source":["Standardize data and train a regression model"]},{"cell_type":"code","metadata":{"id":"g5E6s0DDpVXC"},"source":["### Your code start here ###\n","\n","# search in the sklearn documentation en use 'fit_transform'\n","X_std = \n","\n","lin_reg =\n","### Your code end here ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ex5o7XfRqtuB"},"source":["x_lr = np.array(np.arange(-3, 3, 0.1)).reshape(60, 1)\n","y_lr = lin_reg.predict(x_lr)\n","plt.plot(x_lr, y_lr, color='red')\n","\n","plt.scatter(X,y)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OoBodOk1rsKi"},"source":["Standardize data and train a Support Vector Machine regressor. Help you with the sklearn documentation"]},{"cell_type":"code","metadata":{"id":"p0y-NeMLrqMQ"},"source":["### Your code start here ###\n","\n","# search in the sklearn documentation en use 'fit_transform'\n","X_std = \n","\n","svm = \n","\n","### Your code end here ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"57zXZMSWsAj6"},"source":["x_lr = np.array(np.arange(-3, 3, 0.1)).reshape(60, 1)\n","y_lr = svm.predict(x_lr)\n","plt.plot(x_lr, y_lr, color='red')\n","\n","plt.scatter(X,y)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ey9YLPdmtQH2"},"source":["As you can see SVM can better fit data than linear regression."]},{"cell_type":"markdown","metadata":{"id":"5MWHs_dussi-"},"source":["## 2 Logistic Regression VS SVC"]},{"cell_type":"markdown","metadata":{"id":"cYfWK1yDs8CP"},"source":["You will see the difference between and SVM classifier"]},{"cell_type":"code","metadata":{"id":"IW6LdDBAs2G9"},"source":["from sklearn.datasets import make_moons\n","\n","X, y = make_moons(n_samples=100, noise=0.1)\n","\n","plt.scatter(X[y==0][:,0], X[y==0][:,1], color='blue')\n","plt.scatter(X[y==1][:,0], X[y==1][:,1], color='green')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aH7NnjAotCml"},"source":["Standardize your data en train a logsitic regression."]},{"cell_type":"code","metadata":{"id":"6g5PlpOUs3uT"},"source":["### Your code start here ###\n","\n","# search in the sklearn documentation en use 'fit_transform'\n","X_std = \n","\n","log_reg = \n","\n","### Your code end here ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ex4RZHabtOgq"},"source":["W0 = log_reg.intercept_\n","W = log_reg.coef_\n","\n","rl_x = np.array(range(-1, 3))\n","rl_y = (-1/W[0, 1]) * (rl_x * W[0, 0] + W0[0])\n","\n","plt.plot(rl_x, rl_y, c='red')\n","\n","plt.scatter(X[y==0][:,0], X[y==0][:,1], color='blue')\n","plt.scatter(X[y==1][:,0], X[y==1][:,1], color='green')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xIZH0tQnu_kg"},"source":["Standardize data and train a Support Vector Machine classifier Help you with the sklearn documentation"]},{"cell_type":"code","metadata":{"id":"hlwkkrvRvJaO"},"source":["### Your code start here ###\n","\n","# search in the sklearn documentation en use 'fit_transform'\n","X_std = \n","\n","svm = \n","\n","### Your code end here ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOynNkHuxHv1"},"source":["def make_meshgrid(x, y, h=.02):\n","    \"\"\"Create a mesh of points to plot in\n","\n","    Parameters\n","    ----------\n","    x: data to base x-axis meshgrid on\n","    y: data to base y-axis meshgrid on\n","    h: stepsize for meshgrid, optional\n","\n","    Returns\n","    -------\n","    xx, yy : ndarray\n","    \"\"\"\n","    x_min, x_max = x.min() - 1, x.max() + 1\n","    y_min, y_max = y.min() - 1, y.max() + 1\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                         np.arange(y_min, y_max, h))\n","    return xx, yy\n","  \n","def plot_contours(ax, clf, xx, yy, **params):\n","    \"\"\"Plot the decision boundaries for a classifier.\n","\n","    Parameters\n","    ----------\n","    ax: matplotlib axes object\n","    clf: a classifier\n","    xx: meshgrid ndarray\n","    yy: meshgrid ndarray\n","    params: dictionary of params to pass to contourf, optional\n","    \"\"\"\n","    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    out = ax.contourf(xx, yy, Z, **params)\n","    return out\n","  \n","X0, X1 = X_std[:, 0], X_std[:, 1]\n","xx, yy = make_meshgrid(X0, X1)\n","  \n","plot_contours(plt, svm, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n","plt.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n","plt.xlim(xx.min(), xx.max())\n","plt.ylim(yy.min(), yy.max())\n","plt.xlabel('x2')\n","plt.ylabel('x1')\n","plt.xticks(())\n","plt.yticks(())\n","plt.title('SVM decision boundary')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g2WbRedE4EgS"},"source":["## 3 C hyperpramater in SVM regressor"]},{"cell_type":"code","metadata":{"id":"1ZNmPMF54a4j"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","np.random.seed(123)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7DXib7s54ydw"},"source":["To work we need data, run this piece of code and upload the dataset : univariate_regression.txt"]},{"cell_type":"code","metadata":{"id":"B7KZ-iGp4wUv"},"source":["from google.colab import files\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n","    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"36ik9dmw8cCV"},"source":["svm_data_1 = np.genfromtxt('svm_data_1.txt', delimiter=',')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7C4XAyS8jUt"},"source":["# We construct the X dataset \n","X = svm_data_1[:, :2]\n","\n","# We construct the Y dataset\n","y = svm_data_1[:, 2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ihjFeA01fio8"},"source":["First use what you saw on the Lecture 2 to standardize your data using sklearn."]},{"cell_type":"code","metadata":{"id":"8ibXhRPjfsGN"},"source":["### Your code start here ###\n","\n","\n","# search in the sklearn documentation en use 'fit_transform'\n","X_std = \n","\n","### Your code end here ###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Wsd2bbw_lcl"},"source":["We will begin by with a 2D example dataset which can be separated by a linear boundary. In this dataset, the positions of the positive examples (indicated by a triangle in green) and the negative examples (indicated bya circle in blue) suggest a natural separation indicated by the gap. However, notice that there is an outlier positive example a triangle in green on the far left at about (0.1, 4.1). As part of this exercise, you will also see how this outlier affects the SVM decision boundary."]},{"cell_type":"code","metadata":{"id":"Rriy73TS8w7h"},"source":["plt.scatter(X[y==0, 0], X[y==0, 1], label='Admitted')\n","plt.scatter(X[y==1, 0], X[y==1, 1], label='Not admitted', marker='v')\n","plt.title('Example Dataset 1', size='xx-large')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N931RqeBAfd0"},"source":["In this part of the exercise, you will try using different values of the C parameter with SVMs. Informally, the C parameter is a positive value that controls the penalty for misclassified training examples. A large C parameter tells the SVM to try to classify all the examples correctly. C plays a role similar to $\\frac{1}{\\lambda}$ , where $\\lambda$ is the regularization parameter that we were using previously for logistic regression.\n","\n","New cost function :\n","\n","$$J(W)=C[\\sum^{m}_{i}y^{(i)}cost_1(W^Tx{(i)})+(1-y^{(i)})cost_0(W^Tx^{(i)})]+\\frac{1}{2}\\sum^m_{i=1}w^2_j$$\n","\n","\n","$$C=\\frac{1}{\\lambda}$$"]},{"cell_type":"markdown","metadata":{"id":"9jL3_AmgkLjs"},"source":["Try different value of C and "]},{"cell_type":"code","metadata":{"id":"NU80KKaB9nA7"},"source":["from sklearn.svm import SVC\n","\n","### Your code start here ###\n","svc = SVC(C=1, kernel='linear', gamma='scale')\n","### Your code end here ###\n","\n","svc.fit(X, y)\n","\n","W0 = svc.intercept_\n","W = svc.coef_\n","\n","rl_x = np.array(range(0, 5))\n","rl_y = (-1/W[0, 1]) * (rl_x * W[0, 0] + W0[0])\n","\n","plt.plot(rl_x, rl_y, c='red')\n","\n","plt.scatter(X[y==0, 0], X[y==0, 1], label='Admitted')\n","plt.scatter(X[y==1, 0], X[y==1, 1], label='Not admitted', marker='v')\n","plt.title('SVM Decision Boundary in function of C', size='xx-large')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2jEjwfP9kUUw"},"source":["What do you think about the impact of C in this example ?"]},{"cell_type":"markdown","metadata":{"id":"v2bemr1q0QDh"},"source":["## 4 impact of C in SVM classifier"]},{"cell_type":"code","metadata":{"id":"tUnowXRileF9"},"source":["from sklearn.datasets import make_moons\n","\n","X, y = make_moons(n_samples=100, noise=0.1)\n","\n","plt.scatter(X[y==0][:,0], X[y==0][:,1], color='blue')\n","plt.scatter(X[y==1][:,0], X[y==1][:,1], color='green')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AN9ZqFkz1Ofw"},"source":["Play with C parameter."]},{"cell_type":"code","metadata":{"id":"SGm-EYUilIAc"},"source":["std = StandardScaler()\n","# search in the sklearn documentation en use 'fit_transform'\n","X_std = std.fit_transform(X)\n","\n","### Your code start here ###\n","svm = SVC(C=1, kernel='rbf', gamma='scale')\n","### Your code end here ###\n","\n","svm.fit(X_std, y)\n","\n","X0, X1 = X_std[:, 0], X_std[:, 1]\n","xx, yy = make_meshgrid(X0, X1)\n","  \n","plot_contours(plt, svm, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n","plt.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n","plt.xlim(xx.min(), xx.max())\n","plt.ylim(yy.min(), yy.max())\n","plt.xlabel('x2')\n","plt.ylabel('x1')\n","plt.xticks(())\n","plt.yticks(())\n","plt.title('SVM decision boundary')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DdITIUqu1RJU"},"source":["What do you think about C when it comes large ? And whne it comes small ?"]},{"cell_type":"markdown","metadata":{"id":"nXQygU_H2KHm"},"source":["Test different kernel (sigmoid, poly, linear, etc...)"]}]}